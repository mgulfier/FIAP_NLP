{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from copy import deepcopy\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24730.960917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14277.792868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12366.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37095.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49460.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "count  49459.000000\n",
       "mean   24730.960917\n",
       "std    14277.792868\n",
       "min        1.000000\n",
       "25%    12366.500000\n",
       "50%    24731.000000\n",
       "75%    37095.500000\n",
       "max    49460.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carrega o dataset a partir de uma url da aws\n",
    "df_original = pd.read_csv('https://s3.amazonaws.com/aulas-fiap/imdb-reviews-pt-br.csv')\n",
    "\n",
    "#Exibe as características do dataset\n",
    "df_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            text_en  \\\n",
       "0   1  Once again Mr. Costner has dragged out a movie...   \n",
       "1   2  This is an example of why the majority of acti...   \n",
       "2   3  First of all I hate those moronic rappers, who...   \n",
       "3   4  Not even the Beatles could write songs everyon...   \n",
       "4   5  Brass pictures movies is not a fitting word fo...   \n",
       "\n",
       "                                             text_pt sentiment  \n",
       "0  Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1  Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2  Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3  Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4  Filmes de fotos de latão não é uma palavra apr...       neg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atribui o dataset que será trabalhado \n",
    "df = df_original \n",
    "\n",
    "#converte todas as palavras para minúsculo porque considero que não há diferença entre maiúsculas e minúsculas \n",
    "#para capturar o sentimento contido nas frases\n",
    "df.text_pt = df.text_pt.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vetoriza o texto utilizando TF-IDF em unigramas \n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.text_pt)\n",
    "text_vect = vect.transform(df.text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 80.0670280456543s\n",
      "F1 Score: 0.7077\n"
     ]
    }
   ],
   "source": [
    "#Testa com Árvore de Decisão\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42) \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = tree.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gera um indicador F1 levemente acima do mínimo pedido pelo professor. Mínimo pedido = 70%. Obtido = <b>70,77 %</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.07200050354003906s\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "#Testa com KNN\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "neigh.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = neigh.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtido agora um F1 Score de <b>78%</b>. Melhor que árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 5918.37014746666s\n",
      "F1 Score: 0.8737\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM SVC\n",
    "\n",
    "# ATENÇÃO: Elevado tempo de execução \n",
    "\n",
    "svm_clf = SVC(C=100, kernel='linear',random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_clf.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda melhor, o modelo SVM SVC obteve um F1 Score de <b>87,37%</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 8.073994636535645s\n",
      "F1 Score: 0.8889\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "\n",
    "svm_linear = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor até então, o SVM Linear conseguiu <b>88,89%</b> de F1 Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 102.53799772262573s\n",
      "F1 Score: 0.8441\n"
     ]
    }
   ],
   "source": [
    "#Testa com Random forest\n",
    "\n",
    "rand_forest = RandomForestClassifier(n_estimators=300,random_state=42,max_depth=30)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = rand_forest.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Random Forest não teve um desempenho tão bom, ficou abaixo dos algoritmos de SVM. F1 Score de <b>84,41%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.15800118446350098s\n",
      "F1 Score: 0.857\n"
     ]
    }
   ],
   "source": [
    "#### Teste com Naive Bayes - Bernoulli ####\n",
    "\n",
    "naive_berno = BernoulliNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_berno.fit(X_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_berno.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo da Linha de Naive Bayes (probabilístico) é bem simples e ainda assim conseguiu um desempenho muito bom. \n",
    "F1 Score obtido é de <b>85,70%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.10699868202209473s\n",
      "F1 Score: 0.8617\n"
     ]
    }
   ],
   "source": [
    "## Teste com outro algoritmo probabilístico da família Naive Bayes - Multinomial ###\n",
    "\n",
    "naive_multi = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi.fit(X_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como o outro algoritmo da mesma família, apresenta um bom desempenho e obtem um F1 score ligeiramente superior <b>86,17%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.227791\tvalidation_0-f1:0.771055\n",
      "Multiple eval metrics have been passed: 'validation_0-f1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-f1 hasn't improved in 900 rounds.\n",
      "[1]\tvalidation_0-error:0.191549\tvalidation_0-f1:0.808109\n",
      "[2]\tvalidation_0-error:0.179341\tvalidation_0-f1:0.819855\n",
      "[3]\tvalidation_0-error:0.173023\tvalidation_0-f1:0.826331\n",
      "[4]\tvalidation_0-error:0.170066\tvalidation_0-f1:0.829188\n",
      "[5]\tvalidation_0-error:0.166705\tvalidation_0-f1:0.832621\n",
      "[6]\tvalidation_0-error:0.165946\tvalidation_0-f1:0.833344\n",
      "[7]\tvalidation_0-error:0.160917\tvalidation_0-f1:0.838464\n",
      "[8]\tvalidation_0-error:0.158516\tvalidation_0-f1:0.840838\n",
      "[9]\tvalidation_0-error:0.157808\tvalidation_0-f1:0.841545\n",
      "[10]\tvalidation_0-error:0.155913\tvalidation_0-f1:0.843389\n",
      "[11]\tvalidation_0-error:0.154902\tvalidation_0-f1:0.844428\n",
      "[12]\tvalidation_0-error:0.155028\tvalidation_0-f1:0.844288\n",
      "[13]\tvalidation_0-error:0.155812\tvalidation_0-f1:0.843431\n",
      "[14]\tvalidation_0-error:0.153259\tvalidation_0-f1:0.845993\n",
      "[15]\tvalidation_0-error:0.155028\tvalidation_0-f1:0.844198\n",
      "[16]\tvalidation_0-error:0.156342\tvalidation_0-f1:0.842851\n",
      "[17]\tvalidation_0-error:0.153284\tvalidation_0-f1:0.845978\n",
      "[18]\tvalidation_0-error:0.150984\tvalidation_0-f1:0.848331\n",
      "[19]\tvalidation_0-error:0.150909\tvalidation_0-f1:0.848422\n",
      "[20]\tvalidation_0-error:0.150605\tvalidation_0-f1:0.848688\n",
      "[21]\tvalidation_0-error:0.149165\tvalidation_0-f1:0.850164\n",
      "[22]\tvalidation_0-error:0.150024\tvalidation_0-f1:0.849293\n",
      "[23]\tvalidation_0-error:0.150327\tvalidation_0-f1:0.848997\n",
      "[24]\tvalidation_0-error:0.149999\tvalidation_0-f1:0.849278\n",
      "[25]\tvalidation_0-error:0.147749\tvalidation_0-f1:0.851568\n",
      "[26]\tvalidation_0-error:0.147547\tvalidation_0-f1:0.851791\n",
      "[27]\tvalidation_0-error:0.147016\tvalidation_0-f1:0.852351\n",
      "[28]\tvalidation_0-error:0.146738\tvalidation_0-f1:0.852615\n",
      "[29]\tvalidation_0-error:0.147168\tvalidation_0-f1:0.852162\n",
      "[30]\tvalidation_0-error:0.146814\tvalidation_0-f1:0.852532\n",
      "[31]\tvalidation_0-error:0.145298\tvalidation_0-f1:0.854078\n",
      "[32]\tvalidation_0-error:0.145551\tvalidation_0-f1:0.853823\n",
      "[33]\tvalidation_0-error:0.145778\tvalidation_0-f1:0.853572\n",
      "[34]\tvalidation_0-error:0.145904\tvalidation_0-f1:0.853438\n",
      "[35]\tvalidation_0-error:0.145702\tvalidation_0-f1:0.853621\n",
      "[36]\tvalidation_0-error:0.14593\tvalidation_0-f1:0.853383\n",
      "[37]\tvalidation_0-error:0.145551\tvalidation_0-f1:0.853754\n",
      "[38]\tvalidation_0-error:0.145652\tvalidation_0-f1:0.853634\n",
      "[39]\tvalidation_0-error:0.145702\tvalidation_0-f1:0.853582\n",
      "[40]\tvalidation_0-error:0.144944\tvalidation_0-f1:0.854346\n",
      "[41]\tvalidation_0-error:0.144262\tvalidation_0-f1:0.85503\n",
      "[42]\tvalidation_0-error:0.144717\tvalidation_0-f1:0.85456\n",
      "[43]\tvalidation_0-error:0.143377\tvalidation_0-f1:0.855916\n",
      "[44]\tvalidation_0-error:0.142189\tvalidation_0-f1:0.85713\n",
      "[45]\tvalidation_0-error:0.142998\tvalidation_0-f1:0.856317\n",
      "[46]\tvalidation_0-error:0.142467\tvalidation_0-f1:0.856846\n",
      "[47]\tvalidation_0-error:0.142467\tvalidation_0-f1:0.856839\n",
      "[48]\tvalidation_0-error:0.143023\tvalidation_0-f1:0.856268\n",
      "[49]\tvalidation_0-error:0.141886\tvalidation_0-f1:0.857426\n",
      "[50]\tvalidation_0-error:0.141608\tvalidation_0-f1:0.857725\n",
      "[51]\tvalidation_0-error:0.141734\tvalidation_0-f1:0.857608\n",
      "[52]\tvalidation_0-error:0.142316\tvalidation_0-f1:0.857022\n",
      "[53]\tvalidation_0-error:0.142189\tvalidation_0-f1:0.857147\n",
      "[54]\tvalidation_0-error:0.141911\tvalidation_0-f1:0.857416\n",
      "[55]\tvalidation_0-error:0.142214\tvalidation_0-f1:0.857126\n",
      "[56]\tvalidation_0-error:0.14176\tvalidation_0-f1:0.85758\n",
      "[57]\tvalidation_0-error:0.141608\tvalidation_0-f1:0.857737\n",
      "[58]\tvalidation_0-error:0.141077\tvalidation_0-f1:0.858262\n",
      "[59]\tvalidation_0-error:0.140546\tvalidation_0-f1:0.858799\n",
      "[60]\tvalidation_0-error:0.140091\tvalidation_0-f1:0.859257\n",
      "[61]\tvalidation_0-error:0.139662\tvalidation_0-f1:0.859703\n",
      "[62]\tvalidation_0-error:0.139182\tvalidation_0-f1:0.860183\n",
      "[63]\tvalidation_0-error:0.139182\tvalidation_0-f1:0.860198\n",
      "[64]\tvalidation_0-error:0.138929\tvalidation_0-f1:0.860456\n",
      "[65]\tvalidation_0-error:0.138979\tvalidation_0-f1:0.860395\n",
      "[66]\tvalidation_0-error:0.138954\tvalidation_0-f1:0.860426\n",
      "[67]\tvalidation_0-error:0.138575\tvalidation_0-f1:0.860802\n",
      "[68]\tvalidation_0-error:0.138727\tvalidation_0-f1:0.860634\n",
      "[69]\tvalidation_0-error:0.138803\tvalidation_0-f1:0.860565\n",
      "[70]\tvalidation_0-error:0.138853\tvalidation_0-f1:0.860512\n",
      "[71]\tvalidation_0-error:0.139182\tvalidation_0-f1:0.860178\n",
      "[72]\tvalidation_0-error:0.13951\tvalidation_0-f1:0.859857\n",
      "[73]\tvalidation_0-error:0.139333\tvalidation_0-f1:0.860032\n",
      "[74]\tvalidation_0-error:0.138373\tvalidation_0-f1:0.860996\n",
      "[75]\tvalidation_0-error:0.138575\tvalidation_0-f1:0.860795\n",
      "[76]\tvalidation_0-error:0.137716\tvalidation_0-f1:0.86167\n",
      "[77]\tvalidation_0-error:0.137589\tvalidation_0-f1:0.861788\n",
      "[78]\tvalidation_0-error:0.137387\tvalidation_0-f1:0.861996\n",
      "[79]\tvalidation_0-error:0.137337\tvalidation_0-f1:0.86205\n",
      "[80]\tvalidation_0-error:0.137033\tvalidation_0-f1:0.86236\n",
      "[81]\tvalidation_0-error:0.136528\tvalidation_0-f1:0.86287\n",
      "[82]\tvalidation_0-error:0.136553\tvalidation_0-f1:0.862846\n",
      "[83]\tvalidation_0-error:0.136629\tvalidation_0-f1:0.862769\n",
      "[84]\tvalidation_0-error:0.136174\tvalidation_0-f1:0.863219\n",
      "[85]\tvalidation_0-error:0.136174\tvalidation_0-f1:0.863236\n",
      "[86]\tvalidation_0-error:0.136225\tvalidation_0-f1:0.863192\n",
      "[87]\tvalidation_0-error:0.13668\tvalidation_0-f1:0.862723\n",
      "[88]\tvalidation_0-error:0.136907\tvalidation_0-f1:0.862496\n",
      "[89]\tvalidation_0-error:0.136958\tvalidation_0-f1:0.862444\n",
      "[90]\tvalidation_0-error:0.137059\tvalidation_0-f1:0.862339\n",
      "[91]\tvalidation_0-error:0.13668\tvalidation_0-f1:0.862717\n",
      "[92]\tvalidation_0-error:0.136503\tvalidation_0-f1:0.862891\n",
      "[93]\tvalidation_0-error:0.136831\tvalidation_0-f1:0.862562\n",
      "[94]\tvalidation_0-error:0.13673\tvalidation_0-f1:0.862657\n",
      "[95]\tvalidation_0-error:0.136932\tvalidation_0-f1:0.862449\n",
      "[96]\tvalidation_0-error:0.136831\tvalidation_0-f1:0.862542\n",
      "[97]\tvalidation_0-error:0.136831\tvalidation_0-f1:0.862549\n",
      "[98]\tvalidation_0-error:0.13673\tvalidation_0-f1:0.862656\n",
      "[99]\tvalidation_0-error:0.136882\tvalidation_0-f1:0.862497\n",
      "[100]\tvalidation_0-error:0.13673\tvalidation_0-f1:0.862644\n",
      "[101]\tvalidation_0-error:0.136578\tvalidation_0-f1:0.862796\n",
      "[102]\tvalidation_0-error:0.136528\tvalidation_0-f1:0.862855\n",
      "[103]\tvalidation_0-error:0.136402\tvalidation_0-f1:0.862981\n",
      "[104]\tvalidation_0-error:0.136022\tvalidation_0-f1:0.863363\n",
      "[105]\tvalidation_0-error:0.136149\tvalidation_0-f1:0.863246\n",
      "[106]\tvalidation_0-error:0.13577\tvalidation_0-f1:0.863627\n",
      "[107]\tvalidation_0-error:0.13577\tvalidation_0-f1:0.863621\n",
      "[108]\tvalidation_0-error:0.135542\tvalidation_0-f1:0.863849\n",
      "[109]\tvalidation_0-error:0.135264\tvalidation_0-f1:0.864123\n",
      "[110]\tvalidation_0-error:0.13491\tvalidation_0-f1:0.86448\n",
      "[111]\tvalidation_0-error:0.135163\tvalidation_0-f1:0.864226\n",
      "[112]\tvalidation_0-error:0.134835\tvalidation_0-f1:0.864555\n",
      "[113]\tvalidation_0-error:0.134809\tvalidation_0-f1:0.864579\n",
      "[114]\tvalidation_0-error:0.134986\tvalidation_0-f1:0.864399\n",
      "[115]\tvalidation_0-error:0.134759\tvalidation_0-f1:0.864632\n",
      "[116]\tvalidation_0-error:0.134607\tvalidation_0-f1:0.86478\n",
      "[117]\tvalidation_0-error:0.13438\tvalidation_0-f1:0.86501\n",
      "[118]\tvalidation_0-error:0.134279\tvalidation_0-f1:0.86511\n",
      "[119]\tvalidation_0-error:0.133798\tvalidation_0-f1:0.865596\n",
      "[120]\tvalidation_0-error:0.133748\tvalidation_0-f1:0.865647\n",
      "[121]\tvalidation_0-error:0.133293\tvalidation_0-f1:0.866105\n",
      "[122]\tvalidation_0-error:0.133748\tvalidation_0-f1:0.865648\n",
      "[123]\tvalidation_0-error:0.133546\tvalidation_0-f1:0.865854\n",
      "[124]\tvalidation_0-error:0.133596\tvalidation_0-f1:0.865805\n",
      "[125]\tvalidation_0-error:0.133419\tvalidation_0-f1:0.86598\n",
      "[126]\tvalidation_0-error:0.13304\tvalidation_0-f1:0.866358\n",
      "[127]\tvalidation_0-error:0.132914\tvalidation_0-f1:0.866486\n",
      "[128]\tvalidation_0-error:0.132863\tvalidation_0-f1:0.866543\n",
      "[129]\tvalidation_0-error:0.132408\tvalidation_0-f1:0.866999\n",
      "[130]\tvalidation_0-error:0.132611\tvalidation_0-f1:0.866793\n",
      "[131]\tvalidation_0-error:0.132434\tvalidation_0-f1:0.866973\n",
      "[132]\tvalidation_0-error:0.132358\tvalidation_0-f1:0.867054\n",
      "[133]\tvalidation_0-error:0.132509\tvalidation_0-f1:0.866898\n",
      "[134]\tvalidation_0-error:0.132307\tvalidation_0-f1:0.867104\n",
      "[135]\tvalidation_0-error:0.132408\tvalidation_0-f1:0.866999\n",
      "[136]\tvalidation_0-error:0.13208\tvalidation_0-f1:0.867335\n",
      "[137]\tvalidation_0-error:0.132206\tvalidation_0-f1:0.867207\n",
      "[138]\tvalidation_0-error:0.132231\tvalidation_0-f1:0.867184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139]\tvalidation_0-error:0.132054\tvalidation_0-f1:0.867354\n",
      "[140]\tvalidation_0-error:0.131903\tvalidation_0-f1:0.867509\n",
      "[141]\tvalidation_0-error:0.131726\tvalidation_0-f1:0.867679\n",
      "[142]\tvalidation_0-error:0.131701\tvalidation_0-f1:0.867705\n",
      "[143]\tvalidation_0-error:0.131524\tvalidation_0-f1:0.867881\n",
      "[144]\tvalidation_0-error:0.131372\tvalidation_0-f1:0.868043\n",
      "[145]\tvalidation_0-error:0.130993\tvalidation_0-f1:0.868426\n",
      "[146]\tvalidation_0-error:0.130892\tvalidation_0-f1:0.868526\n",
      "[147]\tvalidation_0-error:0.131018\tvalidation_0-f1:0.868398\n",
      "[148]\tvalidation_0-error:0.130791\tvalidation_0-f1:0.86863\n",
      "[149]\tvalidation_0-error:0.130917\tvalidation_0-f1:0.86851\n",
      "[150]\tvalidation_0-error:0.130766\tvalidation_0-f1:0.868659\n",
      "[151]\tvalidation_0-error:0.130715\tvalidation_0-f1:0.868711\n",
      "[152]\tvalidation_0-error:0.130766\tvalidation_0-f1:0.868656\n",
      "[153]\tvalidation_0-error:0.130437\tvalidation_0-f1:0.868988\n",
      "[154]\tvalidation_0-error:0.130235\tvalidation_0-f1:0.8692\n",
      "[155]\tvalidation_0-error:0.130007\tvalidation_0-f1:0.869425\n",
      "[156]\tvalidation_0-error:0.129932\tvalidation_0-f1:0.869502\n",
      "[157]\tvalidation_0-error:0.129679\tvalidation_0-f1:0.869763\n",
      "[158]\tvalidation_0-error:0.129729\tvalidation_0-f1:0.869712\n",
      "[159]\tvalidation_0-error:0.129982\tvalidation_0-f1:0.869455\n",
      "[160]\tvalidation_0-error:0.12983\tvalidation_0-f1:0.869606\n",
      "[161]\tvalidation_0-error:0.129856\tvalidation_0-f1:0.869581\n",
      "[162]\tvalidation_0-error:0.129932\tvalidation_0-f1:0.8695\n",
      "[163]\tvalidation_0-error:0.129502\tvalidation_0-f1:0.869936\n",
      "[164]\tvalidation_0-error:0.129401\tvalidation_0-f1:0.870042\n",
      "[165]\tvalidation_0-error:0.129148\tvalidation_0-f1:0.870299\n",
      "[166]\tvalidation_0-error:0.128921\tvalidation_0-f1:0.870525\n",
      "[167]\tvalidation_0-error:0.128718\tvalidation_0-f1:0.870726\n",
      "[168]\tvalidation_0-error:0.128693\tvalidation_0-f1:0.87075\n",
      "[169]\tvalidation_0-error:0.128415\tvalidation_0-f1:0.871032\n",
      "[170]\tvalidation_0-error:0.12844\tvalidation_0-f1:0.871007\n",
      "[171]\tvalidation_0-error:0.128188\tvalidation_0-f1:0.871257\n",
      "[172]\tvalidation_0-error:0.128036\tvalidation_0-f1:0.871413\n",
      "[173]\tvalidation_0-error:0.12791\tvalidation_0-f1:0.871539\n",
      "[174]\tvalidation_0-error:0.12791\tvalidation_0-f1:0.871539\n",
      "[175]\tvalidation_0-error:0.127809\tvalidation_0-f1:0.871643\n",
      "[176]\tvalidation_0-error:0.127632\tvalidation_0-f1:0.871821\n",
      "[177]\tvalidation_0-error:0.12748\tvalidation_0-f1:0.871976\n",
      "[178]\tvalidation_0-error:0.127379\tvalidation_0-f1:0.872075\n",
      "[179]\tvalidation_0-error:0.127429\tvalidation_0-f1:0.872026\n",
      "[180]\tvalidation_0-error:0.127126\tvalidation_0-f1:0.872326\n",
      "[181]\tvalidation_0-error:0.127\tvalidation_0-f1:0.872455\n",
      "[182]\tvalidation_0-error:0.126722\tvalidation_0-f1:0.872739\n",
      "[183]\tvalidation_0-error:0.126772\tvalidation_0-f1:0.87269\n",
      "[184]\tvalidation_0-error:0.126469\tvalidation_0-f1:0.872995\n",
      "[185]\tvalidation_0-error:0.126216\tvalidation_0-f1:0.873251\n",
      "[186]\tvalidation_0-error:0.126292\tvalidation_0-f1:0.873174\n",
      "[187]\tvalidation_0-error:0.126039\tvalidation_0-f1:0.873428\n",
      "[188]\tvalidation_0-error:0.125888\tvalidation_0-f1:0.873584\n",
      "[189]\tvalidation_0-error:0.125686\tvalidation_0-f1:0.873787\n",
      "[190]\tvalidation_0-error:0.125761\tvalidation_0-f1:0.873711\n",
      "[191]\tvalidation_0-error:0.12561\tvalidation_0-f1:0.873864\n",
      "[192]\tvalidation_0-error:0.125534\tvalidation_0-f1:0.873945\n",
      "[193]\tvalidation_0-error:0.125408\tvalidation_0-f1:0.874071\n",
      "[194]\tvalidation_0-error:0.125256\tvalidation_0-f1:0.874225\n",
      "[195]\tvalidation_0-error:0.125231\tvalidation_0-f1:0.874254\n",
      "[196]\tvalidation_0-error:0.125079\tvalidation_0-f1:0.874404\n",
      "[197]\tvalidation_0-error:0.125155\tvalidation_0-f1:0.874328\n",
      "[198]\tvalidation_0-error:0.124801\tvalidation_0-f1:0.874684\n",
      "[199]\tvalidation_0-error:0.124498\tvalidation_0-f1:0.87499\n",
      "Tempo de treinamento: 157.73166513442993s\n",
      "F1 Score: 0.7155\n"
     ]
    }
   ],
   "source": [
    "#Teste com o XGBBoost - XGBClassifier #\n",
    "\n",
    "def xgb_f1(y,t):\n",
    "    t = t.get_label()\n",
    "    y_bin = [1. if y_cont > 0.5 else 0. for y_cont in y] # arredondamento para converter para 0. ou 1.\n",
    "    return 'f1',f1_score(t,y_bin,average='weighted')\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=15, learning_rate=0.004,\n",
    "                            n_estimators=200,\n",
    "                            booster='gbtree',\n",
    "                            silent=True,   objective='binary:logistic',\n",
    "                            nthread=-1, gamma=0,\n",
    "                            min_child_weight=1, max_delta_step=0, subsample=0.8,\n",
    "                            colsample_bytree=0.6,\n",
    "                            base_score=0.5,\n",
    "                            seed=0, missing=None)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "xgb_clf.fit(X_train, y_train, eval_metric=xgb_f1,\n",
    "         eval_set=[(X_train, y_train)],\n",
    "         early_stopping_rounds=900)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo XGBClassifier não apresentou um bom desempenho. Apresentou o F1 Score final de apenas <b>71,55%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deste modo, os próximos testes seguirão com os 3 melhores algoritmos considerando o F1 Score:\n",
    "<b>\n",
    "   - SVM Linear:    88,89%\n",
    "   - SVM SVC:       87,37%\n",
    "   - MultinomialNB: 86,17% \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próximo teste: Retirar stop words\n",
    "\n",
    "A próxima estratégia será a retirada de stop words do português utilizando spacy e a execução dos 3 melhores algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando stop words utilizando o spacy \n",
    "# Gerando novamente os vetores de teste  \n",
    "\n",
    "#Carrega base de português do spacy\n",
    "\n",
    "pt = spacy.load('pt_core_news_sm')\n",
    "\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "#Referência as stop words do spacy \n",
    "stop_words_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "#Tokeniza com TF-IDF já excluindo as stop words \n",
    "vect_stop = TfidfVectorizer(ngram_range=(1,1), use_idf=True,stop_words=stop_words_spacy)\n",
    "vect_stop.fit(df.text_pt)\n",
    "text_vect_stop = vect_stop.transform(df.text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz o novo split da nova amostra \n",
    "X_train_stop,X_test_stop,y_train_stop,y_test_stop = train_test_split(\n",
    "    text_vect_stop, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 4.045002460479736s\n",
      "F1 Score: 0.8816\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear mas com stop words já excluídas #\n",
    "\n",
    "svm_linear_stop = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_stop.fit(X_train_stop, y_train_stop)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O teste sem stop words gerou um F1 Score ligeiramente menor <b>88,16%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 4086.915600538254s\n",
      "F1 Score: 0.8647\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM SVC mas com stop words já excluídas\n",
    "\n",
    "# ATENÇÃO: Elevado tempo de execução \n",
    "\n",
    "svm_clf_stop = SVC(C=100, kernel='linear',random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_clf_stop.fit(X_train_stop, y_train_stop)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_clf_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como no caso anterior, um resultado inferior ao apresentado com as stop words. F1 Score = <b>86,47%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.10301756858825684s\n",
      "F1 Score: 0.8611\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB mas com as stop words já excluídas\n",
    "\n",
    "naive_multi_stop = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_stop.fit(X_train_stop,y_train_stop)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_stop.predict(X_test_stop)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stop, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo comportamento dos anteriores. Apresentou aqui um resultado inferior daquele com stop words. F1 Score = <b>86,11%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## Desse modo, a estratégia de excluir as stop words não se mostrou efetiva\n",
    "\n",
    "## A próxima estratégia será utilizar stemizadores e verificar os seus efeitos.\n",
    "\n",
    "## Como o algoritmo SVM SVC tem desempenho consistentemente inferior ao LinearSVC e demora em torno de 2 horas para ser executado, ele será excluído da lista de melhores algoritmos. \n",
    "\n",
    "## Seguiremos com os algoritmos: LinearSVC e MultinomialNB\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\cauee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Realiza o download dos stemizadores rslp e porter\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte cada frase no Dataframe para a sua versão Stemizada pelo RSLPStemmer\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "#Função que converte o texto original para o stematizador RSLP\n",
    "def conv_stem(texto):\n",
    "  return ' '.join([rslp.stem(token) for token in texto.split(' ')])\n",
    "\n",
    "#Cria uma nova coluna no Dataframe com as frases stematizadas\n",
    "df['stemizado'] = df.text_pt.apply(conv_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokeniza as frases stemizadas\n",
    "\n",
    "#Vetoriza o texto utilizando TF-IDF em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.stemizado)\n",
    "text_vect = vect.transform(df.stemizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_stem,X_test_stem,y_train_stem,y_test_stem = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 7.192000150680542s\n",
      "F1 Score: 0.8777\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear as frases lematizadas com RSLP\n",
    "\n",
    "svm_linear_stem = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_stem.fit(X_train_stem, y_train_stem)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_stem.predict(X_test_stem)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado apresentado é inferior àquele com as stop words. O resultado não foi satisfatório. F1 Score atingido = <b>87,77%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.10599970817565918s\n",
      "F1 Score: 0.8546\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "naive_multi_stem = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_stem.fit(X_train_stem,y_train_stem)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_stem.predict(X_test_stem)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo problema com o MultiNomialNB. Não foi satisfatório. F1 Score = <b>85,46%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## Repete os mesmos testes para o stemizador Porter\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria mais uma coluna no Dataframe com as frases stematizadas pelo outro stemizador Porter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Converte frases para o novo stemizador Porter\n",
    "def conv_stem(texto):\n",
    "  return ' '.join([ps.stem(token) for token in texto.split(' ')])\n",
    "\n",
    "#Carrega na nova coluna criada stemizado2\n",
    "df['stemizado2'] = df.text_pt.apply(conv_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vetoriza essa nova stemização\n",
    "\n",
    "#Vetoriza o texto utilizando TF-IDF em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.stemizado2)\n",
    "text_vect = vect.transform(df.stemizado2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_stem2,X_test_stem2,y_train_stem2,y_test_stem2 = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 8.279999017715454s\n",
      "F1 Score: 0.885\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "\n",
    "svm_linear_stem2 = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_stem2.fit(X_train_stem2, y_train_stem2)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_stem2.predict(X_test_stem2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem2, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estratégia não foi bem sucedida. O F1 Score obtido não foi o suficiente. F1 Score obtido = <b>88,50%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.11599946022033691s\n",
      "F1 Score: 0.8583\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "naive_multi_stem2 = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_stem2.fit(X_train_stem2,y_train_stem2)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_stem2.predict(X_test_stem2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_stem2, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo comportamento. F1 Score inferior. Valor obtido = <b>85,83%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## Uma nova estratégia a ser testada é acrescentar informações sintáticas junto às palavras que compõem o texto. \n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de adição do POS Tagger: 3101.849513530731s\n"
     ]
    }
   ],
   "source": [
    "# Acrescentando informação da análise sintática \n",
    "# Gerando novamente os vetores de teste  \n",
    "\n",
    "# ATENÇÃO: Elevado tempo de execução\n",
    "\n",
    "#Converte frase para o seguinte padrão <palavra original>-<classe sintática> \n",
    "def conv_sintatico(texto):\n",
    "    doc = pt(texto)\n",
    "    str = ''\n",
    "    for token in doc:\n",
    "        str += token.text + '-' + token.pos_ + ' '\n",
    "    return str \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#Cria uma nova coluna chamada sintatico com esse padrão\n",
    "df['sintatico'] = df.text_pt.apply(conv_sintatico)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de adição do POS Tagger: \" + str(end - start) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vetoriza os textos com a análise sintática\n",
    "\n",
    "#Vetoriza o texto utilizando TF-IDF em unigramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True)\n",
    "vect.fit(df.sintatico)\n",
    "text_vect = vect.transform(df.sintatico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_sint,X_test_sint,y_train_sint,y_test_sint = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 31.670559406280518s\n",
      "F1 Score: 0.8885\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "\n",
    "svm_linear_sint = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_sint.fit(X_train_sint, y_train_sint)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_sint.predict(X_test_sint)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_sint, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultados não foram suficientes. Ligeiramente menores. F1 Score = <b>88,85%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.11400175094604492s\n",
      "F1 Score: 0.8624\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "naive_multi_sint = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_sint.fit(X_train_sint,y_train_sint)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_sint.predict(X_test_sint)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_sint, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atribui o dataset que será trabalhado \n",
    "df = df_original \n",
    "\n",
    "#converte todas as palavras para minúsculo porque não há diferença entre maiúsculas e minúsculas \n",
    "#para capturar o sentimento contido nas frases\n",
    "df.text_pt = df.text_pt.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## Outro teste realizado foi utilizar Word2Vec de 300 posições a partir de um base carregada da Internet ao invés do TF-IDF como método de vetorização\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz download da base a partir da Internet\n",
    "\n",
    "tar_gz_path = '../extras/cbow_s300.zip'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "  last_block = 0\n",
    "\n",
    "  def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "    self.total = total_size\n",
    "    self.update((block_num - self.last_block) * block_size)\n",
    "    self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "  with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Word2Vec Model') as pbar:\n",
    "    urlretrieve(\n",
    "      'http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s300.zip',\n",
    "      tar_gz_path,\n",
    "      pbar.hook)\n",
    "\n",
    "if not isfile('../extras/cbow_s300.txt'):     \n",
    "  zip_ref = zipfile.ZipFile(tar_gz_path, 'r')\n",
    "  zip_ref.extractall('../extras/')\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento: 227.7136194705963s\n"
     ]
    }
   ],
   "source": [
    "#Tendo a base baixada, carrega para a memória\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_cbow = KeyedVectors.load_word2vec_format('../extras/cbow_s300.txt')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de carregamento: \" + str(end - start) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte uma frase em um único vetor\n",
    "#Faz essa conversão pela soma dos vetores de cada palavras. \n",
    "#A soma será padronizada a partir da raiz quadrada da  média dos elementos ao quadrado\n",
    "\n",
    "def conv_word2vec_frase(frase):\n",
    "    soma =  np.zeros(model_cbow.vector_size)\n",
    "    rms = 0\n",
    "    \n",
    "        \n",
    "    for palavra in frase.split(' '):\n",
    "        palavra = palavra.translate(palavra.maketrans('', '', string.punctuation))    \n",
    "        try:\n",
    "            soma = soma + model_cbow[palavra]    \n",
    "        except:\n",
    "            soma = soma + 0\n",
    "        \n",
    "    rms = 0\n",
    "    \n",
    "    for i in range (model_cbow.vector_size):\n",
    "        rms = rms + (soma[i]*soma[i])\n",
    "            \n",
    "    rms = math.sqrt(rms / model_cbow.vector_size)\n",
    "        \n",
    "    word2vec_frase = soma / rms\n",
    "        \n",
    "            \n",
    "    return word2vec_frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria uma nova coluna na Dataframe para aplicar a conversão de frase em um vetor através do word2vec\n",
    "df['word2vec'] = df.text_pt.apply(conv_word2vec_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte o dataframe em uma matriz\n",
    "n_linhas = df.word2vec.values.shape[0]\n",
    "n_colunas = model_cbow.vector_size\n",
    "\n",
    "word2vec_matriz = np.empty((n_linhas,n_colunas))\n",
    "for i in range(n_linhas):\n",
    "    elemen = df.word2vec[i]\n",
    "    for j in range(n_colunas):\n",
    "        word2vec_matriz[i][j] = elemen[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a divisão da matriz + classificação em amostra de treino e teste\n",
    "\n",
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_cbow,X_test_cbow,y_train_cbow,y_test_cbow = train_test_split(\n",
    "    word2vec_matriz, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 35.94609713554382s\n",
      "F1 Score: 0.7961\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear sobre o dados convertidos via word2vec\n",
    "\n",
    "svm_linear_cbow = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_cbow.fit(X_train_cbow, y_train_cbow)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_cbow.predict(X_test_cbow)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_cbow, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados foram ruins. Foi obtido apenas <b>79,61%</b> de F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.10351777076721191s\n",
      "F1 Score: 0.6736\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "# Como o algoritmo não permite vetores com números negativos\n",
    "# Realizo uma translação no vetor para ficar maior ou igual a zero\n",
    "\n",
    "#Faz translação antes de executar\n",
    "min_train = np.min(X_train_cbow) \n",
    "min_test = np.min(X_test_cbow)\n",
    "\n",
    "X_train_cbow2 = X_train_cbow - min_train\n",
    "X_test_cbow2 = X_test_cbow - min_test\n",
    "\n",
    "naive_multi_cbow = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_cbow.fit(X_train_cbow2,y_train_cbow)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_cbow.predict(X_test_cbow2)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_cbow, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado com o MultiNomialNB é muito pior, ficando abaixo do mínimo solicitado. Apenas <b>67,36%</b> de F1 Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## A estratégia agora será rodar uma rede neural (LSTM) em Keras para verificar se obtem melhor performance que as obtidas até então\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 1300\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df.text_pt.values)\n",
    "X = tokenizer.texts_to_sequences(df.text_pt.values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_keras(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "                 \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "                 \n",
    "                 \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 777, 128)          166400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 777, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 421,594\n",
      "Trainable params: 421,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = [f1_keras])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df.sentiment).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 177s - loss: 0.5509 - f1_keras: 0.7100\n",
      "Epoch 2/15\n",
      " - 172s - loss: 0.3985 - f1_keras: 0.8275\n",
      "Epoch 3/15\n",
      " - 167s - loss: 0.4220 - f1_keras: 0.8218\n",
      "Epoch 4/15\n",
      " - 165s - loss: 0.3952 - f1_keras: 0.8300\n",
      "Epoch 5/15\n",
      " - 166s - loss: 0.3591 - f1_keras: 0.8497\n",
      "Epoch 6/15\n",
      " - 165s - loss: 0.3518 - f1_keras: 0.8518\n",
      "Epoch 7/15\n",
      " - 165s - loss: 0.3474 - f1_keras: 0.8539\n",
      "Epoch 8/15\n",
      " - 165s - loss: 0.3375 - f1_keras: 0.8596\n",
      "Epoch 9/15\n",
      " - 166s - loss: 0.3276 - f1_keras: 0.8642\n",
      "Epoch 10/15\n",
      " - 166s - loss: 0.3207 - f1_keras: 0.8692\n",
      "Epoch 11/15\n",
      " - 166s - loss: 0.3026 - f1_keras: 0.8751\n",
      "Epoch 12/15\n",
      " - 166s - loss: 0.3012 - f1_keras: 0.8765\n",
      "Epoch 13/15\n",
      " - 166s - loss: 0.3087 - f1_keras: 0.8720\n",
      "Epoch 14/15\n",
      " - 166s - loss: 0.3289 - f1_keras: 0.8615\n",
      "Epoch 15/15\n",
      " - 165s - loss: 0.2895 - f1_keras: 0.8811\n",
      "Tempo de treinamento: 2503.5361399650574s\n"
     ]
    }
   ],
   "source": [
    "# ATENÇÃO: Elevado tempo de execução e elevado consumo de memória\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 2)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.8635\n"
     ]
    }
   ],
   "source": [
    "_,score_f1 = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "\n",
    "print(\"f1_score: %.4f\" % (score_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado não foi o suficiente para suplantar os resultados obtidos até então. O F1 Score foi de <b>86,35%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "## Mais uma estratégia, aumentar a quantidade de informações sobre os textos utilizando unigramas e bigramas nos modelos de classificação\n",
    "## Esse enriquecimento será feito nos textos originais (com stop words) que foi o que se mostrou mais promissor até então.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Teste com unigramas e bigramas e os 2 melhores algoritmos ####\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas e digramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), use_idf=True)\n",
    "vect.fit(df.text_pt)\n",
    "text_vect = vect.transform(df.text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_diag,X_test_diag,y_train_diag,y_test_diag = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 10.329999923706055s\n",
      "F1 Score: 0.8949\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "#### MELHOR COM 89,48% ####\n",
    "\n",
    "svm_linear_diag = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_diag.fit(X_train_diag, y_train_diag)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_diag.predict(X_test_diag)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_diag, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excelente resultado !! Melhor F1 Score obtido até então. <b>Novo valor máximo = 89,49%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.38502931594848633s\n",
      "F1 Score: 0.884\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "naive_multi_diag = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_diag.fit(X_train_diag,y_train_diag)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_diag.predict(X_test_diag)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_diag, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito bom resultado também. F1 Score = <b>88,40%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## O teste agora será verificar se a inclusão de trigramas juntamente com unigramas e bigramas aumenta ainda mais o F1 Score\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Teste com unigramas,bigramas e trigramas e os 2 melhores algoritmos ####\n",
    "\n",
    "#Vetoriza o texto utilizando TFID em unigramas e digramas\n",
    "vect = TfidfVectorizer(ngram_range=(1,3), use_idf=True)\n",
    "vect.fit(df.text_pt)\n",
    "text_vect = vect.transform(df.text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treina com a proporção de 80% para treinamento e 20% para teste\n",
    "X_train_trigr,X_test_trigr,y_train_trigr,y_test_trigr = train_test_split(\n",
    "    text_vect, \n",
    "    df.sentiment,\n",
    "    test_size = 0.2, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 16.038594007492065s\n",
      "F1 Score: 0.8909\n"
     ]
    }
   ],
   "source": [
    "#Testa com SVM Linear\n",
    "\n",
    "\n",
    "svm_linear_trigr = LinearSVC(penalty='l1',dual=False,C=1.0, random_state =42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_trigr.fit(X_train_trigr, y_train_trigr)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_trigr.predict(X_test_trigr)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_trigr, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o SVM Linear ocorre uma piora ao acrescentar os trigramas. O F1 Score abaixou para <b>89,09%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 0.743952751159668s\n",
      "F1 Score: 0.887\n"
     ]
    }
   ],
   "source": [
    "# Testa com multinomialNB\n",
    "\n",
    "naive_multi_trigr = MultinomialNB()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "naive_multi_trigr.fit(X_train_trigr,y_train_trigr)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = naive_multi_trigr.predict(X_test_trigr)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_trigr, average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o MultiNomialNB o efeito foi ao contrário, há uma melhoria no F1 Score que alcança o valor de <b>88,70%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## O uso de trigramas não alcançou resultados consistentes para que se prosseguisse com essa estratégia. \n",
    "## Agora serão utilizadas estratégias de melhora dos hiperparâmetros e tentativas de alteração no método de treinamento (K-Fold) com o algoritmo de LinearSVC e a vetorização baseada em unigramas e bigramas\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de treinamento: 231.4320569038391s\n",
      "F1 Score: 0.9097\n"
     ]
    }
   ],
   "source": [
    "#### Com otimização através de GridSearch no classificador LinearSVC ####\n",
    "#### MELHOR 90,97% de f1 score #####\n",
    "\n",
    "parametros = {'penalty': ['l1', 'l2'],\n",
    "              'C': [1.0,2.0,4.0] }\n",
    "\n",
    "\n",
    "svm_linear_opt = GridSearchCV(svm_linear_diag, parametros, scoring='f1_weighted')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "svm_linear_opt.fit(X_train_diag, y_train_diag)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "y_prediction = svm_linear_opt.predict(X_test_diag)\n",
    "\n",
    "f1 = f1_score(y_prediction, y_test_diag, average='weighted')\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o modelo otimizado pela otimização dos hiperparâmetros (GridSearchCV). <b>Obtemos o novo melhor máximo = 90,97%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 obtido em treinamento...\n",
      "0.9050582560296846\n",
      "f1 obtido em treinamento...\n",
      "0.9121243504860276\n",
      "f1 obtido em treinamento...\n",
      "0.9040877191251182\n",
      "f1 obtido em treinamento...\n",
      "0.9000145928641914\n",
      "f1 obtido em treinamento...\n",
      "0.9181900836650582\n",
      "f1 obtido em treinamento...\n",
      "0.9111263506992067\n",
      "f1 obtido em treinamento...\n",
      "0.9181847401076808\n",
      "f1 obtido em treinamento...\n",
      "0.9120420703618893\n",
      "f1 obtido em treinamento...\n",
      "0.916082851939068\n",
      "f1 obtido em treinamento...\n",
      "0.9261962199588266\n",
      "f1 obtido em treinamento...\n",
      "0.8928208121917877\n",
      "f1 obtido em treinamento...\n",
      "0.8999078942935429\n",
      "f1 obtido em treinamento...\n",
      "0.90495974719088\n",
      "f1 obtido em treinamento...\n",
      "0.918149196895327\n",
      "f1 obtido em treinamento...\n",
      "0.9100414049792268\n",
      "f1 obtido em treinamento...\n",
      "0.8928269489599285\n",
      "f1 obtido em treinamento...\n",
      "0.912051067362934\n",
      "f1 obtido em treinamento...\n",
      "0.9059856237327915\n",
      "f1 obtido em treinamento...\n",
      "0.910029252819553\n",
      "f1 obtido em treinamento...\n",
      "0.8969140500331428\n",
      "f1 obtido em treinamento...\n",
      "0.919110046937198\n",
      "f1 obtido em treinamento...\n",
      "0.9039473056159063\n",
      "f1 obtido em treinamento...\n",
      "0.8978983655592682\n",
      "f1 obtido em treinamento...\n",
      "0.8989092709176287\n",
      "f1 obtido em treinamento...\n",
      "0.9100266741691917\n",
      "f1 obtido em treinamento...\n",
      "0.9009341281781241\n",
      "f1 obtido em treinamento...\n",
      "0.9110376117709905\n",
      "f1 obtido em treinamento...\n",
      "0.9140570609470082\n",
      "f1 obtido em treinamento...\n",
      "0.9171137462173915\n",
      "f1 obtido em treinamento...\n",
      "0.9150985610556642\n",
      "f1 obtido em treinamento...\n",
      "0.9049630516458137\n",
      "f1 obtido em treinamento...\n",
      "0.9160900612052514\n",
      "f1 obtido em treinamento...\n",
      "0.912051067362934\n",
      "f1 obtido em treinamento...\n",
      "0.919110046937198\n",
      "f1 obtido em treinamento...\n",
      "0.9140913475556394\n",
      "f1 obtido em treinamento...\n",
      "0.9089988028043479\n",
      "f1 obtido em treinamento...\n",
      "0.9029326517592178\n",
      "f1 obtido em treinamento...\n",
      "0.9029624345959705\n",
      "f1 obtido em treinamento...\n",
      "0.9231785940551434\n",
      "f1 obtido em treinamento...\n",
      "0.9010104153077676\n",
      "F1 final com a amostra de teste....\n",
      "F1 Score: 0.9095\n"
     ]
    }
   ],
   "source": [
    "# ATENÇÃO: Elevado tempo de execução \n",
    "\n",
    "# Teste do melhor método com K-FOLD \n",
    "X_kfold = X_train_diag\n",
    "#Y_kfold = y_train.as_matrix()\n",
    "Y_kfold = y_train_diag.values\n",
    "    \n",
    "kf = StratifiedKFold(n_splits=40,random_state=42,shuffle=True)\n",
    "\n",
    "clf = svm_linear_opt\n",
    "\n",
    "\n",
    "best_model = None \n",
    "best_f1 = -1 \n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "for train_index, test_index in kf.split(X_kfold,Y_kfold):  \n",
    "    X_train_kfold, X_test_kfold = X_kfold[train_index], X_kfold[test_index]\n",
    "    y_train_kfold, y_test_kfold = Y_kfold[train_index], Y_kfold[test_index]\n",
    "    \n",
    "    \n",
    "    clf.fit(X_train_kfold, y_train_kfold)\n",
    "    y_prediction = clf.predict(X_test_kfold)\n",
    "    f1 = f1_score(y_prediction, y_test_kfold, average='weighted')\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "       best_f1 = f1\n",
    "       best_model = deepcopy(clf)\n",
    "    print(\"f1 obtido em treinamento...\")    \n",
    "    print(f1)\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"Tempo de treinamento: \" + str(end - start) + \"s\")\n",
    "\n",
    "X_final_test = X_test_diag \n",
    "#Y_final_test = y_test_diag.as_matrix()\n",
    "Y_final_test = y_test_diag.values\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_final_test)\n",
    "\n",
    "print(\"F1 final com a amostra de teste....\")\n",
    "f1 = f1_score(y_pred,Y_final_test,average='weighted')\n",
    "\n",
    "print(\"F1 Score: \" + str(round(f1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Mesmo utilizando treinamento com a estratégia de K-FOLD não se conseguiu superar o modelo otimizado com divisão simples de amostras de treinamento e validação. Nesse último teste, obteve-se F1 Score = <b>90,95%</b>. Quase o mesmo valor obtido anteriormente, mas ainda assim inferior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "# Resultado Final\n",
    "\n",
    "# Melhor algoritmo: SVM - LinearSVC\n",
    "\n",
    "# Melhor estratégia: Vetorizar utilizando unigramas e bigramas e TF-IDF\n",
    "\n",
    "# Otimização: Tuning dos hiperparâmetros do modelo LinearSVC utilizando GridSearchCV\n",
    "\n",
    "# F1 Score final: 90,97%\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
